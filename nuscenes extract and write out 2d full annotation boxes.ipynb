{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncreated by @asha\\nmarch 8th 2019\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "created by @asha\n",
    "march 8th 2019\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v0.1 ...\n",
      "23 category,\n",
      "8 attribute,\n",
      "5 visibility,\n",
      "6975 instance,\n",
      "12 sensor,\n",
      "1200 calibrated_sensor,\n",
      "304715 ego_pose,\n",
      "12 log,\n",
      "100 scene,\n",
      "3977 sample,\n",
      "304715 sample_data,\n",
      "99952 sample_annotation,\n",
      "12 map,\n",
      "Done loading in 9.8 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 2.6 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "# Let's start by initializing the database\n",
    "%matplotlib inline\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "import numpy as np\n",
    "\n",
    "nusc = NuScenes(version='v0.1', dataroot='data/nuscenes', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categories that are annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human.pedestrian.adult\n",
      "human.pedestrian.child\n",
      "human.pedestrian.wheelchair\n",
      "human.pedestrian.stroller\n",
      "human.pedestrian.personal_mobility\n",
      "human.pedestrian.police_officer\n",
      "human.pedestrian.construction_worker\n",
      "animal\n",
      "vehicle.car\n",
      "vehicle.motorcycle\n",
      "vehicle.bicycle\n",
      "vehicle.bus.bendy\n",
      "vehicle.bus.rigid\n",
      "vehicle.truck\n",
      "vehicle.construction\n",
      "vehicle.emergency.ambulance\n",
      "vehicle.emergency.police\n",
      "vehicle.trailer\n",
      "movable_object.barrier\n",
      "movable_object.trafficcone\n",
      "movable_object.pushable_pullable\n",
      "movable_object.debris\n",
      "static_object.bicycle_rack\n"
     ]
    }
   ],
   "source": [
    "# The NuScenes class holds several tables. Each table is a list of records, and each record is a dictionary. \n",
    "# For example the first record of the category table is stored at\n",
    "\n",
    "#nusc.category[0]['name']\n",
    "\n",
    "#these are the categories available\n",
    "cat = []\n",
    "for i in range(len(nusc.category)):\n",
    "    print(nusc.category[i]['name'])\n",
    "    cat.append(nusc.category[i]['name'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classes that we are detecting :\n",
    "\n",
    "We merge adult, child, police officer, construction worker into a single class called pedestrian\n",
    "We are detecting: \n",
    "- pedestrian\n",
    "- car \n",
    "- bicycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['human.pedestrian.adult', 'human.pedestrian.child','human.pedestrian.police_officer','human.pedestrian.construction_worker','vehicle.car','vehicle.bicycle']\n",
    "pedestrians = ['human.pedestrian.adult', 'human.pedestrian.child','human.pedestrian.police_officer','human.pedestrian.construction_worker'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples\n",
      "3977\n"
     ]
    }
   ],
   "source": [
    "print('Total number of samples')\n",
    "print(len(nusc.sample))\n",
    "\n",
    "total_no_of_samples = len(nusc.sample)\n",
    "\n",
    "#print('Total number of images')\n",
    "#print(len(nusc.sample*6)) #6 different cameras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defined the following function:\n",
    "\n",
    "- get_sample_data (edit of nutonomy's original nusc.get_sample_data)\n",
    "        \n",
    "        input:(nusc, sample_data_token)\n",
    "        output:path to the data, lists of 3d bounding boxes in the image (in camera coordinates), \n",
    "        annotation token of annotations in the image, intrinsic matrix of the camera)\n",
    "        \n",
    "\n",
    "- threeD_to_2D\n",
    "         \n",
    "        input: (box (camera coordinates),intrinsic matrix))\n",
    "        output : corners of the 2d bounding box in image plane\n",
    "\n",
    "- all_3d_to_2d(boxes,anns,intrinsic)\n",
    "\n",
    "        input : boxes in camera coordinates, list of annotation tokens of annotations in the image, \n",
    "        intrinsic matrix\n",
    "        output: x_min,x_max,y_min,y_max,width,height of the 2D boundings boxes of objects that are\n",
    "        more than 40% visible in panoramic view of all cameras, also ensures that the center of the \n",
    "        bounding boxes falls inside the image\n",
    "\n",
    "- extract_bounding_box(i):\n",
    "        \n",
    "        input: sample number\n",
    "        output: min x, max x, min y max y, width and height of bounding box in image coordinates \n",
    "        2d bounding box of objects which are 40% visible in panoramic view of all cameras and center \n",
    "        falls witin the image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyquaternion import Quaternion\n",
    "from nuscenes.utils.data_classes import Box\n",
    "from nuscenes.utils.geometry_utils import quaternion_slerp, box_in_image, BoxVisibility\n",
    "import numpy as np\n",
    "def get_sample_data(nusc_object, sample_data_token, box_vis_level=BoxVisibility.ANY, selected_anntokens=None):\n",
    "    \"\"\"\n",
    "    Returns the data path as well as all annotations related to that sample_data(single image).\n",
    "    Note that the boxes are transformed into the current sensor's coordinate frame.\n",
    "    :param sample_data_token: <str>. Sample_data token(image token).\n",
    "    :param box_vis_level: <BoxVisibility>. If sample_data is an image, this sets required visibility for boxes.\n",
    "    :param selected_anntokens: [<str>]. If provided only return the selected annotation.\n",
    "    :return: (data_path <str>, boxes [<Box>], camera_intrinsic <np.array: 3, 3>)\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve sensor & pose records\n",
    "    sd_record = nusc_object.get('sample_data', sample_data_token)\n",
    "    cs_record = nusc_object.get('calibrated_sensor', sd_record['calibrated_sensor_token'])\n",
    "    sensor_record = nusc_object.get('sensor', cs_record['sensor_token'])\n",
    "    pose_record = nusc_object.get('ego_pose', sd_record['ego_pose_token'])\n",
    "\n",
    "    sample_record = nusc_object.get('sample',sd_record['sample_token'])\n",
    "    data_path = nusc_object.get_sample_data_path(sample_data_token)\n",
    "\n",
    "    if sensor_record['modality'] == 'camera':\n",
    "        cam_intrinsic = np.array(cs_record['camera_intrinsic'])\n",
    "        imsize = (sd_record['width'], sd_record['height'])\n",
    "    else:\n",
    "        cam_intrinsic = None\n",
    "        imsize = None\n",
    "\n",
    "    # Retrieve all sample annotations and map to sensor coordinate system.\n",
    "    if selected_anntokens is not None:\n",
    "        boxes = list(map(nusc_object.get_box, selected_anntokens))\n",
    "    else:\n",
    "        boxes = nusc_object.get_boxes(sample_data_token)\n",
    "        selected_anntokens = sample_record['anns']\n",
    "\n",
    "    # Make list of Box objects including coord system transforms.\n",
    "    box_list = []\n",
    "    ann_list = []\n",
    "    for box,ann in zip(boxes,selected_anntokens):\n",
    "\n",
    "        # Move box to ego vehicle coord system\n",
    "        box.translate(-np.array(pose_record['translation']))\n",
    "        box.rotate(Quaternion(pose_record['rotation']).inverse)\n",
    "\n",
    "        #  Move box to sensor coord system\n",
    "        box.translate(-np.array(cs_record['translation']))\n",
    "        box.rotate(Quaternion(cs_record['rotation']).inverse)\n",
    "\n",
    "        if sensor_record['modality'] == 'camera' and not \\\n",
    "                box_in_image(box, cam_intrinsic, imsize, vis_level=box_vis_level):\n",
    "            continue\n",
    "\n",
    "        box_list.append(box)\n",
    "        ann_list.append(ann)\n",
    "    #this is for a single sample image\n",
    "    return data_path, box_list, ann_list, cam_intrinsic #single image info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threeD_2_twoD(boxsy,intrinsic): #input is a single annotation box\n",
    "    '''\n",
    "    given annotation boxes and intrinsic camera matrix\n",
    "    outputs the 2d bounding box coordinates as a list (all annotations for a particular sample image)\n",
    "    '''\n",
    "    corners = boxsy.corners()\n",
    "    x = corners[0,:]\n",
    "    y = corners[1,:]\n",
    "    z = corners[2,:]\n",
    "    x_y_z = np.array((x,y,z))\n",
    "    orthographic = np.dot(intrinsic,x_y_z)\n",
    "    perspective_x = orthographic[0]/orthographic[2]\n",
    "    perspective_y = orthographic[1]/orthographic[2]\n",
    "    perspective_z = orthographic[2]/orthographic[2]\n",
    "    \n",
    "    min_x = np.min(perspective_x)\n",
    "    max_x = np.max(perspective_x)\n",
    "    min_y = np.min(perspective_y)\n",
    "    max_y = np.max(perspective_y)\n",
    "    \n",
    "\n",
    "    \n",
    "    return min_x,max_x,min_y,max_y\n",
    "\n",
    "\n",
    "\n",
    "def all_3d_to_2d(boxes,anns,intrinsic): #input 3d boxes, annotation key lists, intrinsic matrix (one image)\n",
    "    x_min=[]\n",
    "    x_max=[]\n",
    "    y_min=[]\n",
    "    y_max =[]\n",
    "    width=[]\n",
    "    height=[]\n",
    "    objects_detected =[]\n",
    "    orig_objects_detected =[]\n",
    "    \n",
    "   \n",
    "    for j in range(len(boxes)): #iterate through boxes\n",
    "        box=boxes[j]\n",
    "        \n",
    "        if box.name in classes: #if the box.name is in the classes we want to detect\n",
    "        \n",
    "            if box.name in pedestrians: \n",
    "                orig_objects_detected.append(\"pedestrian\")\n",
    "            elif box.name == \"vehicle.car\":\n",
    "                orig_objects_detected.append(\"car\")\n",
    "            else:\n",
    "                orig_objects_detected.append(\"cyclist\")\n",
    "            #print(box)\n",
    "            \n",
    "            visibility = nusc.get('sample_annotation', '%s' %anns[j])['visibility_token'] #give annotation key\n",
    "            visibility = int(visibility)\n",
    "\n",
    "            \n",
    "            if visibility > 1: #more than 40% visible in the panoramic view of the the cameras\n",
    "\n",
    "                    \n",
    "                center = box.center #get boxe's center\n",
    "\n",
    "                center = np.dot(intrinsic,center)\n",
    "                center_point = center/(center[2]) #convert center point into image plane\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                if center_point[0] <-100 or center_point[0] > 1700 or center_point[1] <-100 or center_point[1] >1000:\n",
    "                    #if center of bounding box is outside of the image, do not annotate\n",
    "                    pass\n",
    "                \n",
    "                else:\n",
    "                    min_x, max_x, min_y, max_y = threeD_2_twoD(box,intrinsic) #converts box into image plane\n",
    "                    w = max_x - min_x\n",
    "                    h = max_y - min_y\n",
    "        \n",
    "        \n",
    "                    x_min.append(min_x)\n",
    "                    x_max.append(max_x)\n",
    "                    y_min.append(min_y)\n",
    "                    y_max.append(max_y)\n",
    "                    width.append(w)\n",
    "                    height.append(h)\n",
    "                    if box.name in pedestrians: \n",
    "                        objects_detected.append(\"pedestrian\")\n",
    "                    elif box.name == \"vehicle.car\":\n",
    "                        objects_detected.append(\"car\")\n",
    "                    else:\n",
    "                        objects_detected.append(\"cyclist\")\n",
    "                    \n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    return x_min,x_max,y_min,y_max,width,height,objects_detected,orig_objects_detected #for a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bounding_box(i,camera_name): #give a single sample number and camera name\n",
    "    \n",
    "    '''\n",
    "    input sample number i, camera name\n",
    "    outputs min x, max x, min y max y, width and height of bounding box in image coordinates\n",
    "    2d bounding box\n",
    "    options for camera name : CAM_FRONT, CAM_FRONT_RIGHT, CAM_FRONT_LEFT, CAM_BACK, CAM_BACK_RIGHT,CAM_BACK_LEFT\n",
    "    '''\n",
    "    \n",
    "    nusc.sample[i] #one image\n",
    "    \n",
    "    camera_token = nusc.sample[i]['data']['%s' %camera_name] #one camera, get the camera token \n",
    "\n",
    "    path, boxes, anns, intrinsic_matrix = get_sample_data(nusc,'%s' %camera_token) #gets data for one image\n",
    "    \n",
    "    x_min, x_max,y_min,y_max,width,height, objects_detected,orig_objects_detected = all_3d_to_2d(boxes,anns, intrinsic_matrix)\n",
    "    \n",
    "    return x_min, x_max, y_min, y_max, width, height, path, boxes,intrinsic_matrix, objects_detected,orig_objects_detected\n",
    "    #info for a single image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create target Directory if don't exist\n",
    "import os.path\n",
    "def create_annotation_directory(camera):\n",
    "    current_dir =os.getcwd()\n",
    "    #current_dir =\"%s/annotation\" %pwd\n",
    "    dirName =\"%s/annotation/%s_anno\" %(current_dir,camera)\n",
    "    if not os.path.exists(dirName):\n",
    "        os.makedirs(dirName)\n",
    "        print(\"Directory \" , dirName ,  \" Created \")\n",
    "    else:    \n",
    "        print(\"Directory \" , dirName ,  \" already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree as ET\n",
    "def write_xml_annotation(x_min,x_max,y_min,y_max,width,height,path,boxes,objects_detected): #single image info\n",
    "    #detected_items =[]\n",
    "    #import xml.etree.cElementTree as ET\n",
    "    path_split = path.split(\"/\")\n",
    "    full_image_name = path_split[-1]\n",
    "    name =full_image_name.split(\".\")[0]\n",
    "    \n",
    "    root = ET.Element(\"annotation\")\n",
    "\n",
    "\n",
    "    ET.SubElement(root, \"folder\").text = \"%s\" %camera\n",
    "    ET.SubElement(root, \"filename\").text = \"%s\" %full_image_name\n",
    "    ET.SubElement(root, \"path\").text = \"%s\" %path\n",
    "\n",
    "    source = ET.SubElement(root, \"source\")\n",
    "    ET.SubElement(source, \"database\").text = \"nuTonomy-nuscenes\"\n",
    "\n",
    "    size = ET.SubElement(root, \"size\")\n",
    "    ET.SubElement(size, \"width\").text=\"1600\"\n",
    "    ET.SubElement(size,\"height\").text=\"900\"\n",
    "    ET.SubElement(size,\"depth\").text=\"3\"\n",
    "    ET.SubElement(root, \"segmented\").text = \"0\"\n",
    "\n",
    "    for j in range(len(objects_detected)): #\n",
    "        \n",
    "        ob= ET.SubElement(root, \"object\")\n",
    "        ET.SubElement(ob,\"name\").text=\"%s\" %objects_detected[j]\n",
    "        ET.SubElement(ob,\"pose\").text=\"Unspecified\"\n",
    "        ET.SubElement(ob, \"truncated\").text=\"truncated\"\n",
    "        ET.SubElement(ob, \"difficult\").text=\"0\"\n",
    "\n",
    "        bb = ET.SubElement(ob,\"bndbox\")\n",
    "        ET.SubElement(bb,\"xmin\").text=\"%s\" %x_min[j]\n",
    "        ET.SubElement(bb,\"ymin\").text=\"%s\" %y_min[j]\n",
    "        ET.SubElement(bb,\"xmax\").text=\"%s\" %x_max[j]\n",
    "        ET.SubElement(bb,\"ymax\").text=\"%s\" %y_max[j]\n",
    "        \n",
    " \n",
    "    filename = \"%s/%s.xml\" %(dirName,name)\n",
    "    tree = ET.ElementTree(root)\n",
    "    #tree.write(\"%s/%s.xml\" %(dirName,name),pretty_print=True)\n",
    "    tree.write(\"%s\" %filename, pretty_print=True)\n",
    "    \n",
    "    return filename #file a single file\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAM_FRONT\n",
      "Directory  /Volumes/Luthor/nutonomy/nuscenes-devkit-master/python-sdk/annotation/CAM_FRONT_anno  Created \n",
      "CAM_FRONT_RIGHT\n",
      "Directory  /Volumes/Luthor/nutonomy/nuscenes-devkit-master/python-sdk/annotation/CAM_FRONT_RIGHT_anno  Created \n",
      "CAM_FRONT_LEFT\n",
      "Directory  /Volumes/Luthor/nutonomy/nuscenes-devkit-master/python-sdk/annotation/CAM_FRONT_LEFT_anno  Created \n",
      "CAM_BACK\n",
      "Directory  /Volumes/Luthor/nutonomy/nuscenes-devkit-master/python-sdk/annotation/CAM_BACK_anno  Created \n",
      "CAM_BACK_RIGHT\n",
      "Directory  /Volumes/Luthor/nutonomy/nuscenes-devkit-master/python-sdk/annotation/CAM_BACK_RIGHT_anno  Created \n",
      "CAM_BACK_LEFT\n",
      "Directory  /Volumes/Luthor/nutonomy/nuscenes-devkit-master/python-sdk/annotation/CAM_BACK_LEFT_anno  Created \n"
     ]
    }
   ],
   "source": [
    "camera_names =['CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_FRONT_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT', 'CAM_BACK_LEFT']\n",
    "i = 0\n",
    "detected_items =[]\n",
    "orig_detected_items=[]\n",
    "obs = []\n",
    "\n",
    "file=[]\n",
    "\n",
    "for camera in camera_names: #iterate through all cameras\n",
    "    print(camera)\n",
    "    create_annotation_directory(camera)\n",
    "    current_dir =os.getcwd()\n",
    "    dirName =\"%s/annotation/%s_anno\" %(current_dir,camera) #current directory's name\n",
    "    #we are looking at one camera now\n",
    "    for sample_number in range(total_no_of_samples):#look at a single image\n",
    "        #print(sample_number)\n",
    "        #get in for a single image\n",
    "        \n",
    "        x_min, x_max,y_min,y_max,width,height, path, boxes, intrinsic_matrix,objects_detected,orig_objects_detected = extract_bounding_box(sample_number, '%s' %camera) \n",
    "        write_xml_annotation(x_min,x_max,y_min,y_max,width,height,path,boxes,objects_detected)\n",
    "         \n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
